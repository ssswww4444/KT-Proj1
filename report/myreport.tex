\documentclass[11pt]{article}
\usepackage{colacl}
\sloppy

\title{Waht kinda typoz do poeple mak?}
\author
{Anonymous}

\begin{document}
\maketitle

\section{Introduction}

\paragraph{} 

Most of the recent spelling correction systems are based on Winnow algorithm (Machine Learning), probability scoring (Bayesian), Neural Networks, Levenshtein Edit Distance, etc. 

The goal of this report is to determine what kind of typographical errors people make. In this report, one baseline algorithm and two advanced algorithms will be implemented for comparisons and evaluations.

\subsection{Dataset}

\paragraph{} The dataset used in this report involves 4453 common misspelling errors made by the editors of Wikipedia\cite{WikiMisspell}, and their corresponding truly intended spellings.

\begin{table}[h]
 \begin{center}
\begin{tabular}{| l | l | l |}

      \hline
      Evaluation Metric & 20\% & 100\% \\
      \hline\hline
      Precision & 0.2700 & 0.2604 \\
      Recall & 0.7899 & 0.7905 \\
      \hline

\end{tabular}
\caption{Compare 20\% and 100\% of dataset}\label{table1}
 \end{center}
\end{table}

According to the evaluation metrics of the baseline algorithm shown above (results are rounded to 4 decimal places), there is no much difference between 20\% and 100\% of the dataset. Therefore, only 20\% random selected tokens of the dataset (890 tokens) will be used for the rest of the algorithms.

All the tokens in the dataset and words in the dictionary are lower-cased, therefore, no preprocessing is required for the spelling correction.

\subsection{Evaluation Metrics}

\paragraph{} In this report, the algorithms applied will give multiple predictions for each misspelled word, therefore, precision and recall will be used as the evaluation metrics.

In order to compare between the baseline and advanced algorithms, precision and recall can be combined into a single evaluation metric called F-Score, which is the harmonic mean of precision and recall.\cite{InfromationRetrieval}

$$ 
F_1  = \frac{2}{\frac{1}{recall} + \frac{1}{precision}}
$$

\section{Hypothesis}

\paragraph{} The possible types of typographical error could be: 

\begin{enumerate}
\item
Transposition of two adjacent characters

\item
Substitution of a truly intended character to a wrong character

\item
Duplication of characters
\end{enumerate}

This paper will only focus on the three types of typographical errors listed above, which could be tested with Damerau–Levenshtein Distance, Weighted–Levenshtein Distance, and N-Gram Distance respectively.

\section{Method}

\subsection{Levenshtein Distance (LD)}
\paragraph{} The Levenshtein Distance (LD) gives the Global Edit Distance (GED) between the misspelled words and the dictionary entries with parameter (m,i,d,r) = (0,1,1,1). 

This method is used as a baseline method. The comparisons of the results between this baseline algorithm and the other algorithms indicate the presence or absence of the corresponding types of typographical errors.

\begin{table}[h]
 \begin{center}
\begin{tabular}{| l | l |}

      \hline
      Evaluation Metric & LD \\
      \hline\hline
      Precision & 0.2700 \\
      Recall & 0.7899 \\
      F-Score & 0.4024 \\
      \hline

\end{tabular}
\caption{Evaluation of Levenshtein Distance}\label{table2}
 \end{center}
\end{table}


\subsection{Damerau–Levenshtein Distance (DLD)}

\paragraph{} Damerau–Levenshtein Distance (DLD) is very similar to LD, but it also takes the transposition of two adjacent characters into account, and treat transposition as an operation with cost 1.

This additional character operation allows the DLD algorithm to give the misspelled tokens with transposition error a lower distance to truly intended word in the dictionary.

\begin{table}[h]
 \begin{center}
\begin{tabular}{| l | l |}

      \hline
      Evaluation Metric & DLD \\
      \hline\hline
      Precision & 0.3422 \\
      Recall & 0.8551 \\
      F-Score & 0.4888 \\
      \hline

\end{tabular}
\caption{Evaluation of Damerau-Levenshtein Distance}\label{table3}
 \end{center}
\end{table}

\subsection{Weighted–Levenshtein Distance (WLD)}

\paragraph{} Weighted-Levenshtein is another type of Levenshtein Edit Distance (or Global Edit Distance) which allows us to modify the replace cost for a particular character to another character (default cost = 1).

\subsubsection{Parameters}

\paragraph{} By analysing the dataset, the most common substitutions are shown at the table below.

\begin{table}[h]
 \begin{center}
\begin{tabular}{| l | l | l |}

      \hline
      Wrong Char & True Char & Frequency \\
      \hline\hline
	 a & e & 231 \\     
	 e & a & 185 \\
      i & e & 129 \\
      e & i & 127 \\
      a & i & 125 \\
      e & o & 83 \\
      i & a & 73 \\
      \hline

\end{tabular}
\caption{Substitution frequency in Wikipedia dataset (show frequency greter than 70 only)}\label{table4}
 \end{center}
\end{table}

\subsubsection{Implementation}

\paragraph{} For the implementation, the replace cost of the top 5 frequent: (a,e), (e,a), (i,e), (e,i), (a,i), were setted to 0.5, while the cost of others were remained at 1.

\begin{table}[h]
 \begin{center}
\begin{tabular}{| l | l |}

      \hline
      Evaluation Metric & WLD \\
      \hline\hline
      Precision & 0.3168 \\
      Recall & 0.7618 \\
      F-Score & 0.4475 \\
      \hline

\end{tabular}
\caption{Evaluation of Weighted-Levenshtein Distance}\label{table5}
 \end{center}
\end{table}

\subsection{N-Gram}

\paragraph{} The N-Gram distance is also known as Q-Gram distance, the distance between n-grams of string \textit{s} and \textit{t} can be calculated by the following equation:
$$
	|G_n(s)|+ |G_n(t)| - |G_n(s) \cap G_n(t)|
$$
where $G_n(s)$ and $G_n(t)$ are the n-grams of string \textit{s} and \textit{t} respectively.

The bigram (N=2) method could be useful for the spelling correction, according to the hypothesis: ``Duplication of characters", 

\begin{table}[h]
 \begin{center}
\begin{tabular}{| l | l |}

      \hline
      Evaluation Metric & N-Gram \\
      \hline\hline
      Precision & 0.4815 \\
      Recall & 0.6854 \\
      F-Score & 0.5656 \\
      \hline

\end{tabular}
\caption{Evaluation of Damerau-Levenshtein Distance}\label{table3}
 \end{center}
\end{table}

\section{Discussion}

\begin{table}[h]
\begin{center}
\begin{tabular}{| l | l |}

      \hline
      Method & F-Score \\
      \hline\hline
      LD & 0.4024 \\
      DLD & 0.4888 \\
      WLD & 0.4475 \\
      N-Gram &  0.5656 \\
      \hline
\end{tabular}

\caption{Comparing LD with other methods}\label{table5}
\end{center}
\end{table}


\section{Conclusion}

Concluding text.

\bibliographystyle{acl}
\bibliography{mybib}

\end{document}