{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import Levenshtein as levenshtein\n",
    "import json\n",
    "import fuzzy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import soundex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all files and store into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    lines = f.readlines()\n",
    "    ls = []\n",
    "    for line in lines:\n",
    "        ls.append(line.strip())\n",
    "    f.close()\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ls = readfile(\"dict.txt\")\n",
    "correct_ls = readfile(\"wiki_correct.txt\")\n",
    "misspell_ls = readfile(\"wiki_misspell.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best match(es) for the word in dictionary according to the global edit distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_match_levenshtein(target):\n",
    "    # init best matches\n",
    "    min_dist = levenshtein.distance(target,dict_ls[0])\n",
    "    best_matches = [dict_ls[0]]\n",
    "    \n",
    "    for word in dict_ls[1:]:\n",
    "        if abs(len(word) - len(target)) > min_dist:  # not possible to be min_dist, skip\n",
    "            continue\n",
    "        dist = levenshtein.distance(target,word)  # cal global edit distance\n",
    "        # replace if shorter distance\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_matches = [word]\n",
    "        elif dist == min_dist:\n",
    "            best_matches.append(word)\n",
    "    \n",
    "    return best_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best matches for the words in wiki data sets according to levenshtein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levenshtein_correction_dict = {}\n",
    "for word in wiki_misspell_ls:\n",
    "    if word not in levenshtein_correction_dict: # avoid repeated word\n",
    "        levenshtein_correction_dict[word] = best_match_levenshtein(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save correction correction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('levenshtein_correction.json', 'w') as fp:\n",
    "    json.dump(levenshtein_correction_dict, fp)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('levenshtein_correction.json', 'r') as fp:\n",
    "    levenshtein_correction_dict = json.load(fp)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the random selected mispelled subset (1335 tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspell_subset, misspell_left, correct_subset, correct_left = train_test_split(misspell_ls, \n",
    "                                                                                correct_ls,\n",
    "                                                                                train_size = 0.3,\n",
    "                                                                                test_size = 0.7,\n",
    "                                                                                shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine two subsets into a list and save as json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_subset = [(misspell_subset), (correct_subset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('misspell_correct_subset.json', 'w') as fp:\n",
    "    json.dump(combined_subset, fp)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('misspell_correct_subset.json', 'r') as fp:\n",
    "    combined_subset = json.load(fp)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mispell_subset = combined_subset[0]\n",
    "correct_subset = combined_subset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best matches for the words in wiki data subset according to levenshtein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neigbour\n",
      "neighbour\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-9793ca69ffa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmisspell_subset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubcorrection_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# avoid repeated word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msubcorrection_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_match_levenshtein\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-6ac9e8d25a62>\u001b[0m in \u001b[0;36mbest_match_levenshtein\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlevenshtein\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# cal global edit distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# replace if shorter distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_dist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mmin_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mbest_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subcorrection_dict = {}\n",
    "for word in misspell_subset:\n",
    "    if word not in subcorrection_dict: # avoid repeated word\n",
    "        subcorrection_dict[word] = best_match_levenshtein(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save correction correction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ged_sub_correction.json', 'w') as fp:\n",
    "    json.dump(sub_correction_dict, fp)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ged_sub_correction.json', 'r') as fp:\n",
    "    sub_correction_dict = json.load(fp)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metric Implementation (Precision and Recall and F-Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_eval(result_dict, curr_misspell_ls, curr_correct_ls):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    for i in range(len(curr_misspell_ls)):\n",
    "        word = curr_misspell_ls[i]\n",
    "        true_word = curr_correct_ls[i]\n",
    "        if true_word in result_dict[word]:\n",
    "            tp += 1\n",
    "            fp += len(result_dict[word]) - 1\n",
    "        else:\n",
    "            fn += 1 # miss\n",
    "            fp += len(result_dict[word])\n",
    "            \n",
    "    precision = tp*1.0 / (tp + fp)\n",
    "    recall = tp*1.0 / (tp + fn)\n",
    "    fscore = 2*(precision*recall)/(precision + recall)\n",
    "\n",
    "    return precision,recall,fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, recall, and F-score for levenshtein (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2604320804971885, 0.7904783292162587, 0.3917858534142134)\n"
     ]
    }
   ],
   "source": [
    "print wiki_eval(levenshtein_correction_dict, misspell_ls, correct_ls)\n",
    "# print len(wiki_misspell_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GED implementation with customized parameter set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform s1 to s2# trans \n",
    "def edit_distance(s1,s2, params):\n",
    "    # Levenshteiin Distance (match,insert,delete,replace): the lower the better\n",
    "    (m,i,d,r) = params\n",
    "    \n",
    "    # init matrix\n",
    "    s1_len = len(s1)\n",
    "    s2_len = len(s2)\n",
    "    A = np.zeros((s2_len+1,s1_len+1))\n",
    "    A[0][0] = 0\n",
    "    for j in range(1, s2_len+1):\n",
    "        A[j][0] = j*i;  # insert\n",
    "    for k in range(1, s1_len+1):\n",
    "        A[0][k] = k*d;  # delete\n",
    "        \n",
    "    # filling in table\n",
    "    for j in range(1,s2_len+1):\n",
    "        for k in range(1,s1_len+1):\n",
    "            A[j][k] = min(A[j][k-1] + d,\n",
    "                          A[j-1][k] + i,\n",
    "                          A[j-1][k-1] + equal(s1[k-1],s2[j-1],m,r))\n",
    "    return A[s2_len,s1_len]\n",
    "    \n",
    "def equal(ch1,ch2,cost1,cost2):\n",
    "    if ch1 == ch2:\n",
    "        return cost1\n",
    "    else:\n",
    "        return cost2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edit_distance(\"crat\",\"arts\",(0,1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best match(es) for the word in dictionary according to the soundex table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_match_soundex(target):\n",
    "    \n",
    "    soundex = fuzzy.Soundex(4)\n",
    "    target_soundex = soundex(target)\n",
    "    \n",
    "    best_matches = []\n",
    "    \n",
    "    for word in dict_ls:\n",
    "        print word\n",
    "        print soundex(word)\n",
    "        if (soundex(word) == target_soundex):  # same soundex\n",
    "            best_matches.append(word)\n",
    "    print best_matches\n",
    "    \n",
    "    return best_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_correction_dict_soundex = {}\n",
    "for word in wiki_misspell_ls:\n",
    "    if word not in wiki_correction_dict_soundex:\n",
    "        wiki_correction_dict_soundex[word] = best_match_soundex(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundex = fuzzy.Soundex(4)\n",
    "soundex(\"aam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print wiki_dict_ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
