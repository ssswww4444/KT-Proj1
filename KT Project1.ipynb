{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import modules & Read files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Levenshtein as levenshtein\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jellyfish\n",
    "from weighted_levenshtein import lev as weighted_lev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to read the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    lines = f.readlines()\n",
    "    ls = []\n",
    "    for line in lines:\n",
    "        ls.append(line.strip())\n",
    "    f.close()\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read dicitonary and wiki dataset and store as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ls = readfile(\"dict.txt\")\n",
    "correct_ls = readfile(\"wiki_correct.txt\")\n",
    "misspell_ls = readfile(\"wiki_misspell.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to evaluate the spelling correction results. (Precision & Recall & F-Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_eval(result_dict, curr_misspell_ls, curr_correct_ls):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    for i in range(len(curr_misspell_ls)):\n",
    "        word = curr_misspell_ls[i]\n",
    "        true_word = curr_correct_ls[i]\n",
    "        if true_word in result_dict[word]:\n",
    "            tp += 1\n",
    "            fp += len(result_dict[word]) - 1\n",
    "        else:\n",
    "            fn += 1 # miss\n",
    "            fp += len(result_dict[word])\n",
    "            \n",
    "    precision = tp*1.0 / (tp + fp)\n",
    "    recall = tp*1.0 / (tp + fn)\n",
    "    fscore = 2*(precision*recall)/(precision + recall)\n",
    "\n",
    "    return precision,recall,fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Baseline method: Levenshtein Distance (LD)\n",
    "## 3.1. Run with the whole Wikipedia Dataset (4453 tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to find the best matches for the misspelled words according to the levenshtein distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_match_LD(target):\n",
    "    # init best match as first entry of the dict\n",
    "    min_dist = levenshtein.distance(target,dict_ls[0])\n",
    "    best_matches = [dict_ls[0]]\n",
    "    \n",
    "    for word in dict_ls[1:]:\n",
    "        if abs(len(word) - len(target)) > min_dist:  # not possible to be min_dist, skip\n",
    "            continue\n",
    "        dist = levenshtein.distance(target,word)  # cal global edit distance\n",
    "        # replace if shorter distance\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_matches = [word]\n",
    "        elif dist == min_dist:\n",
    "            best_matches.append(word)\n",
    "    \n",
    "    return best_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best matches for the words in wiki data sets according to levenshtein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "LD_correction_dict = {}\n",
    "for word in misspell_ls:\n",
    "    if word not in LD_correction_dict: # avoid repeated word\n",
    "        LD_correction_dict[word] = best_match_LD(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the correction results to numpy file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.save('levenshtein_results.npy', LD_correction_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the results from numpy file if required.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "LD_correction_dict = np.load('levenshtein_results.npy').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, recall, and F-score for Levenshtein (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics (Levenshtein Distance, 100% Data)\n",
      "---------------------------------------------------\n",
      "Precision: 0.260432080497\n",
      "Recall: 0.790478329216\n",
      "F-score: 0.391785853414\n"
     ]
    }
   ],
   "source": [
    "(precision, recall, fscore) = wiki_eval(LD_correction_dict, misspell_ls, correct_ls)\n",
    "print \"Evaluation Metrics (Levenshtein Distance, 100% Data)\"\n",
    "print \"---------------------------------------------------\"\n",
    "print \"Precision: \" + str(precision)\n",
    "print \"Recall: \" + str(recall)\n",
    "print \"F-score: \" + str(fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Run with only 20% of the Wikipedia Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Randomly extract 20% of the Wikipedia Dataset as a subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly extract the subset with \"train_test_split\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspell_subset, misspell_left, correct_subset, correct_left = train_test_split(misspell_ls, \n",
    "                                                                                correct_ls,\n",
    "                                                                                train_size = 0.2,\n",
    "                                                                                test_size = 0.8,\n",
    "                                                                                shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine two subsets into a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_subset = [(misspell_subset), (correct_subset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the combined subset to numpy file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.save('combined_subset.npy', combined_subset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the combined subset from numpy file and store as variables if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "combined_subset = np.load('combined_subset.npy')\n",
    "\n",
    "# store as variables\n",
    "misspell_subset = combined_subset[0]\n",
    "correct_subset = combined_subset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Run Levenshtein with the data subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best matches for the words in data subset according to Levenshtein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "LD_subcorrection_dict = {}\n",
    "for word in misspell_subset:\n",
    "    if word not in LD_subcorrection_dict: # avoid repeated word\n",
    "        LD_subcorrection_dict[word] = best_match_LD(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the correction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.save('subset_levenshtein_results.npy', LD_subcorrection_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "LD_subcorrection_dict = np.load('subset_levenshtein_results.npy').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, recall, and F-score for Levenshtein (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics (Levenshtein Distance, 20% Data)\n",
      "---------------------------------------------------\n",
      "Precision: 0.269969278034\n",
      "Recall: 0.789887640449\n",
      "F-score: 0.402404121351\n"
     ]
    }
   ],
   "source": [
    "(precision, recall, fscore) = wiki_eval(LD_subcorrection_dict, misspell_subset, correct_subset)\n",
    "print \"Evaluation Metrics (Levenshtein Distance, 20% Data)\"\n",
    "print \"---------------------------------------------------\"\n",
    "print \"Precision: \" + str(precision)\n",
    "print \"Recall: \" + str(recall)\n",
    "print \"F-score: \" + str(fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Damerau-Levenshtein Distance (DLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to find the best matches of the misspelled words with DLD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_match_DLD(target):\n",
    "    # init best match as first entry of the dict\n",
    "    min_dist = jellyfish.damerau_levenshtein_distance(unicode(target,\"utf-8\"),unicode(dict_ls[0],\"utf-8\"))\n",
    "    best_matches = [dict_ls[0]]\n",
    "    \n",
    "    for word in dict_ls[1:]:\n",
    "        if abs(len(word) - len(target)) > min_dist:  # not possible to be min_dist, skip\n",
    "            continue\n",
    "        dist = jellyfish.damerau_levenshtein_distance(unicode(target,\"utf-8\"),unicode(word,\"utf-8\"))  # cal DLD\n",
    "        # replace if shorter distance\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_matches = [word]\n",
    "        elif dist == min_dist:\n",
    "            best_matches.append(word)\n",
    "\n",
    "    return best_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best matches for the wiki misspelled words according to DLD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "DLD_correction_dict = {}\n",
    "for word in misspell_subset:\n",
    "    if word not in DLD_correction_dict: # avoid repeated word\n",
    "        DLD_correction_dict[word] = best_match_DLD(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the results of DLD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.save('DLD_results.npy', DLD_correction_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "DLD_correction_dict = np.load('DLD_results.npy').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, recall, and F-score for DLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics (Damerau-Levenshtein Distance, 20% Data)\n",
      "---------------------------------------------------\n",
      "Precision: 0.342176258993\n",
      "Recall: 0.855056179775\n",
      "F-score: 0.488760436737\n"
     ]
    }
   ],
   "source": [
    "(precision, recall, fscore) = wiki_eval(DLD_correction_dict, misspell_subset, correct_subset)\n",
    "print \"Evaluation Metrics (Damerau-Levenshtein Distance, 20% Data)\"\n",
    "print \"---------------------------------------------------\"\n",
    "print \"Precision: \" + str(precision)\n",
    "print \"Recall: \" + str(recall)\n",
    "print \"F-score: \" + str(fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Weighted-Levenshtein Distance (WLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find replacement errors in the wikipedia dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_tuple_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(misspell_ls)):\n",
    "    misspell = misspell_ls[i] \n",
    "    correct = correct_ls[i]\n",
    "    if len(misspell) == len(correct): # same length\n",
    "        diff = 0\n",
    "        diff_tuple = None\n",
    "        for j in range(len(misspell)):\n",
    "            if misspell[j] != correct[j]:\n",
    "                diff += 1\n",
    "                diff_tuple = (misspell[j],correct[j])\n",
    "        if diff == 1:  # add to dict\n",
    "            if diff_tuple not in replace_tuple_dict:\n",
    "                replace_tuple_dict[diff_tuple] = 1\n",
    "            else:\n",
    "                replace_tuple_dict[diff_tuple] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print replacement error with frequency greater than 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('e', 'a'), 185)\n",
      "(('a', 'e'), 231)\n",
      "(('i', 'a'), 73)\n",
      "(('e', 'o'), 83)\n",
      "(('i', 'e'), 129)\n",
      "(('a', 'i'), 125)\n",
      "(('e', 'i'), 127)\n"
     ]
    }
   ],
   "source": [
    "for item in replace_tuple_dict.items():\n",
    "    if item[1] > 70:  # print freq > 70 only\n",
    "        print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define substitute costs according to the frequency observed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitute_costs = np.ones((128, 128), dtype=np.float64)  # make a 2D array of 1's\n",
    "substitute_costs[ord('e'), ord('a')] = 0.5  # make substituting 'a' for 'e' cost 0.5\n",
    "substitute_costs[ord('a'), ord('e')] = 0.5\n",
    "substitute_costs[ord('i'), ord('e')] = 0.5\n",
    "substitute_costs[ord('a'), ord('i')] = 0.5\n",
    "substitute_costs[ord('e'), ord('i')] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_match_WLD(target):\n",
    "    # init best match as first entry of the dict\n",
    "    min_dist = weighted_lev(target, dict_ls[0], substitute_costs=substitute_costs)\n",
    "    best_matches = [dict_ls[0]]\n",
    "    \n",
    "    for word in dict_ls[1:]:\n",
    "        dist = weighted_lev(target, word, substitute_costs=substitute_costs)   # cal WLD\n",
    "        # replace if shorter distance\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_matches = [word]\n",
    "        elif dist == min_dist:\n",
    "            best_matches.append(word)\n",
    "\n",
    "    return best_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "WLD_correction_dict = {}\n",
    "for word in misspell_subset:\n",
    "    if word not in WLD_correction_dict: # avoid repeated word\n",
    "        WLD_correction_dict[word] = best_match_WLD(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.save('WLD_results.npy', WLD_correction_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "WLD_correction_dict = np.load('WLD_results.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics (Weighted-Levenshtein Distance, 20% Data)\n",
      "---------------------------------------------------\n",
      "Precision: 0.316822429907\n",
      "Recall: 0.761797752809\n",
      "F-score: 0.447524752475\n"
     ]
    }
   ],
   "source": [
    "(precision, recall, fscore) = wiki_eval(WLD_correction_dict, misspell_subset, correct_subset)\n",
    "print \"Evaluation Metrics (Weighted-Levenshtein Distance, 20% Data)\"\n",
    "print \"---------------------------------------------------\"\n",
    "print \"Precision: \" + str(precision)\n",
    "print \"Recall: \" + str(recall)\n",
    "print \"F-score: \" + str(fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. N-Gram Distance (N=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to get all the bigrams of a word as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_dict(word):\n",
    "    bigram_dict = {}\n",
    "    new_word = \"#\" + word + \"#\"  # padding\n",
    "    \n",
    "    for i in range(len(new_word)-1):\n",
    "        gram = new_word[i:i+2]\n",
    "        if gram not in bigram_dict:\n",
    "            bigram_dict[gram] = 1\n",
    "        else:\n",
    "            bigram_dict[gram] += 1\n",
    "        \n",
    "    return bigram_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to calculate the bigram distance between the ngram dictionaries s_dict and t_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_dist(s_dict,t_dict, s_len, t_len):\n",
    "    s_grams = s_dict.keys()\n",
    "    t_grams = t_dict.keys()\n",
    "    \n",
    "    intersect = 0\n",
    "    \n",
    "    for gram in s_grams:\n",
    "        if gram in t_grams:\n",
    "            intersect += min(s_dict[gram], t_dict[gram])\n",
    "            \n",
    "    return ((s_len+1) + (t_len+1) - 2*intersect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the ngram dictionaries for each misspelled token & dictionary entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram dictionaries\n",
    "misspell_subset_dict = []\n",
    "dict_ls_dict = []\n",
    "\n",
    "for word in misspell_subset:\n",
    "    misspell_subset_dict.append(get_bigram_dict(word))\n",
    "\n",
    "for word in dict_ls:\n",
    "    dict_ls_dict.append(get_bigram_dict(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to find the best matches for a misspelled token with its index in the subset given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_match_ngram(target_index):\n",
    "    # init best match as first entry of the dict\n",
    "    target_dict = misspell_subset_dict[target_index]\n",
    "    target_len = len(misspell_subset)\n",
    "    min_dist = bigram_dist(target_dict,dict_ls_dict[0], target_len, len(dict_ls[0]))\n",
    "    best_matches = [dict_ls[0]]\n",
    "    \n",
    "    for i in range(1,len(dict_ls)):\n",
    "        dist = bigram_dist(target_dict,dict_ls_dict[i], target_len, len(dict_ls[i]))\n",
    "        # replace if shorter distance\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_matches = [dict_ls[i]]\n",
    "        elif dist == min_dist:\n",
    "            best_matches.append(dict_ls[i])\n",
    "\n",
    "    return best_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best matches for the wiki misspelled words according to N-Gram (N=2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_correction_dict = {}\n",
    "for i in range(len(misspell_subset)):\n",
    "    word = misspell_subset[i]\n",
    "    if word not in ngram_correction_dict: # avoid repeated word\n",
    "        ngram_correction_dict[word] = best_match_ngram(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.save('ngram_results.npy', ngram_correction_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "ngram_correction_dict = np.load('ngram_results.npy').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, recall, and F-score for N-Gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics (N-Gram Distance, 20% Data)\n",
      "---------------------------------------------------\n",
      "Precision: 0.481452249408\n",
      "Recall: 0.685393258427\n",
      "F-score: 0.565600370885\n"
     ]
    }
   ],
   "source": [
    "(precision, recall, fscore) = wiki_eval(ngram_correction_dict, misspell_subset, correct_subset)\n",
    "print \"Evaluation Metrics (N-Gram Distance, 20% Data)\"\n",
    "print \"---------------------------------------------------\"\n",
    "print \"Precision: \" + str(precision)\n",
    "print \"Recall: \" + str(recall)\n",
    "print \"F-score: \" + str(fscore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
